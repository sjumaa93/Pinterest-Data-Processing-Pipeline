# Pinterest-Data-Processing-Pipeline

Build the system that Pinterest uses to analyse both historical, and real-time data generated by posts from their users.

Pinterest has world-class machine learning engineering systems. They have billions of user interactions such as image uploads or image clicks which they need to process every day to inform the decisions to make. In this project, I built the system in the cloud that takes in those events and runs them through two separate pipelines. One for computing real-time metrics (like profile popularity, which would be used to recommend that profile in real-time), and another for computing metrics that depend on historical data (such as the most popular category this year). 

# Data Ingestion: Apache Kafka
- The data source is an infinite posting loop, which simulates a users activity on-site and posts the JSONs to a localhost port.
- API/project_pin_API.py contains an API to receive the user posts, it also sets up the kafka producer to add the posts to the topic.
- batch_consumer.py creates the Kafka consumer, and passes all messages it finds in the topic to an s3 bucket.

# Batch Processing: Apache Spark, Apache Cassandra, Presto
- A spark session reads the data from the s3 bucket, that data is stored in a spark dataframe. With some initial batch cleaning such as:
    - Converting follower count into a real number, e.g. 10k into 10,000.
    - Dropping colomns with information that isn't useful.
    - Converting strings into integers where neccasary.
- Cassandra was setup and configure Cassandra locally ready to receive the data from Spark for long term storage.
- Cassandra is then inegrated with Presto, a powerful data querying engine. This allows us to run ad-hoc queries on the data we have so far.

# Batch Monitoring
- A JMX exporter allowed me to connect Cassandra with prometheus. This was then visualized in a Grafana dashboard in order to minotor cassandras instance attributes.

# Streaming

# Storage

# Monitoring